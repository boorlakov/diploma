\section{ВВЕДЕНИЕ В ИСКУССТВЕННЫЕ НЕЙРОННЫЕ СЕТИ}
Обработка естественного языка является крайне тяжелой задачей для моделирования стандартными алгоритмами. Машинное обучение позволяет решать задачи на основе статистических наблюдений из данных без явной алгоритмизации решения задачи. Недавние прорывы в области обработки естественного языка показывают, что методами машинного обучения можно частично или сполна выполнять многие человеческие задачи, например, краткое изложение текста, написание кода, общение с собеседником и другие, а также добиться результатов распознавания речи сопоставимых с результатами человека \cite{human-wer,whisper}.

Одним из основных аспектов машинного обучения является искусственная нейронная сеть (далее ИНС), созданная по подобию биологических нейронных сетей. Модель ИНС -- описание сети, математическая модель, часто представляемая в виде графа, нацеленная на решение задачи прогнозирования на основе обучающей выборки данных. Методами настройки параметров моделей под конкретную задачу называют методами обучения. Такими методами являются: обучение с учителем, обучение без учителя и обучение с подкреплением.

Каждый метод имеет свои особенности и применяется в зависимости от ситуации. Например, обычно обучение с учителем применяется в тех случаях, когда обучающий набор данных размечен на основе некоторых критериев. Такие задачи обычно являются задачами классификации, когда каждый экземпляр выборки имеет один или больше собственный класс. Такой подход имеет ограничения: как правило количество размеченных данных значительно меньше общего количество данных. В ситуациях, когда данные не размечены, применяется обучение без учителя. Благодаря такому подходу, можно обучить модель делить данные на кластеры, генерировать текст, изображения и т.д. Когда модели приходится принимать решения как интеллектуальный агент в условиях данной ей среды и соотвествующих откликов среды на решения, применяется метод обучения с подкреплением. Для построения мощных современных цифровых ассистенов могут использоваться все три подхода к обучению моделей, используя модели, полученных конкретным методом, в качестве промежуточных или вспомогательных, для обучения конечной модели \cite{state-of-gpt}.

\subsection{УСТРОЙСТВО ПРОСТОЙ ИСКУССТВЕННОЙ НЕЙРОННОЙ СЕТИ}

Устройство простой ИНС можно описать как взвешенный набор узлов, разделенный на слои, соединенные между собой активационными функциями $\varphi$. Функциям активации желательно иметь свойства: нелинейность, непрерывная дифференцируемость, бесконечная область значений, монотонность. При построении модели ИНС в качестве активационных функций часто используется одна из следующих функций:
\begin{enumerate}
    \item Гиперболический тангенс:
          \begin{equation}
              \varphi(z) = \frac{e^{2z}-1}{e^{2z}+1}.
          \end{equation}
    \item Функция ReLU:
          \begin{equation}
              \varphi(z) = \max(0, z).
              \label{relu}
          \end{equation}
    \item Функция GELU:
          \begin{equation}
              \varphi(z) = \frac{1}{2}z\left[1+\text{erf}\left({z}/\sqrt{2}\right)\right].
              \label{gelu}
          \end{equation}
    \item Логистическая функция (сигмоида):
          \begin{equation}
              \varphi(z) = \frac{1}{1+e^{-z}}.
          \end{equation}
    \item Многопеременная логистическая функция (softmax):
          \begin{equation}
              \varphi(z)_i = \frac{e^{z_i}}{\sum_{i=1}^{K}e^{z_j}}.
          \end{equation}
\end{enumerate}

Архитектуры ИНС могут сильно отличаться друг от друга в зависимости от поставленных задач и требований к качеству предсказаний модели. Раздел, который занимается изучением ИНС с большим количеством скрытых слоев, т.е. тех слоев, которые находятся между входным и выходным, называется глубоким обучением, а такие модели называются глубокими. Примером такой архитектуры модели может служить  трансформер \cite{transformer-paper}, речь о котором пойдет дальше.

Набор весов $W$ и отклонений $b$ являются параметрами модели, обозначаемые как $\theta$. Прогноз или функция гипотезы модели ИНС обозначается как $h_{\theta}$. $W^{[l]}$, $b^{[l]}$, $h_{\theta}^{[l]}$ -- веса, отклонения и выход модели на $l$-ом слое. Описать работу обобщенной модели ИНС c $L$ слоями можно следующим образом:
\begin{enumerate}
    \item $h_{\theta}^{[0]} = x$.
    \item $h_{\theta}^{[l]} = \varphi \circ \left(W^{[l-1]}h_{\theta}^{[l-1]}(x) + b^{[l-1]}\right), \text{где } 1 \le l \le L-1$.
    \item $h_{\theta} = h_{\theta}^{[L]} = W^{[L-1]}h_{\theta}^{[L-1]}(x) + b^{[L-1]}$.
\end{enumerate}

Примером простой ИНС может являться однослойный перцептрон. Схема однослойного перцептрона представлена на рис. \ref{fig:perceptron}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            init/.style={
                    draw,
                    circle,
                    inner sep=2pt,
                    font=\Huge,
                    join = by -latex
                },
            squa/.style={
                    draw,
                    inner sep=2pt,
                    font=\Large,
                    join = by -latex
                },
            start chain=2,node distance=17mm
        ]
        \node[on chain=2]
        (x2) {$x_2$};
        \node[on chain=2,join=by o-latex]
        {$w_2$};
        \node[on chain=2,init] (sigma)
        {$\displaystyle\Sigma$};
        \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Функция \\ активации}}]
        {$\varphi$};
        \node[on chain=2,label=above:Выход,join=by -latex]
        {$y$};
        \begin{scope}[start chain=1]
            \node[on chain=1] at (0,1.5cm)
            (x1) {$x_1$};
            \node[on chain=1,join=by o-latex]
            (w1) {$w_1$};
        \end{scope}
        \begin{scope}[start chain=3]
            \node[on chain=3] at (0,-1.5cm)
            (x3) {$x_3$};
            \node[on chain=3,label=below:Веса,join=by o-latex]
            (w3) {$w_3$};
        \end{scope}
        \node[label=above:\parbox{2cm}{\centering Отклонение\\$b$}] at (sigma|-w1) (b) {};

        \draw[-latex] (w1) -- (sigma);
        \draw[-latex] (w3) -- (sigma);
        \draw[o-latex] (b) -- (sigma);

        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Вход} (x3.south west);
    \end{tikzpicture}
    \caption{Однослойный перцептрон}
    \label{fig:perceptron}
\end{figure}

\subsection{ОБУЧЕНИЕ С УЧИТЕЛЕМ ИСКУССТВЕННОЙ НЕЙРОННОЙ СЕТИ}

Как было сказано раннее, для того, чтобы обучить ИНС с учителем, требуется иметь такой набор данных, где каждый элемент имел соответствующую метку класса. Элементы набора данных, т.е. входные данные, принадлежат некоторому входному пространству $\mathcal{X}$, например, картинкам кошек, а метки принадлежат к выходному пространству $\mathcal{Y}$, например, породе кошек. Из такого набора данных $\mathcal{D}$ мы строим тренировочную подвыборку, состоящую из пар, элементов:

\begin{equation}
    \mathcal{D}_{\text{train}} = \{\,(x_i, \hat y_i) \mid x_i \in \mathcal{X}, \hat y_i \in \mathcal{Y}, i=\overline{1, \dots, n}, n \le \lvert \mathcal{D} \rvert\,\}.
\end{equation}

Мы стремимся получить целевую функцию ИНС $h_{\theta^*}$ с оптимальным набором параметров $\theta^*$ на основе $\mathcal{D}_{\text{train}}$, при котором $h_{\theta^*}$ наиболее эффективно отображает из пространства $\mathcal{X}$ в пространство $\mathcal{Y}$. Для определения того, насколько эффективно предсказывает модель, требуется иметь неотрицательную функцию $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^+$, которая измеряет ошибку предсказания $h_{\theta}(x)$ по отношению к истинной метке $\hat y$. Такие функции, как правило, называются функциями ошибки или функциями потерь. Функция потерь выбирается исходя из условий конкретной задачи, но часто является одной из следующих функций:
\begin{enumerate}
    \item Функция потерь $L_1$:
          \begin{equation}
              \ell\left(h_\theta(x), \hat y\right) = \lvert \hat y - h_\theta(x) \rvert
          \end{equation}
    \item Функция потерь $L_2$:
          \begin{equation}
              \ell\left(h_\theta(x), \hat y\right) = \big(\hat y - h_\theta(x)\big)^2
          \end{equation}
    \item Функция потерь перекрестной энтропии:
          \begin{equation}
              \ell\left(h_\theta(x), \hat y\right) = - \hat y\log{h_\theta(x)}
          \end{equation}
    \item Функция потерь NLL:
          \begin{equation}
              \ell\left(h_\theta(x), \hat y\right) = -\left[\hat y \log{h_\theta(x)} + (1 - \hat y)\log(1 - h_\theta(x))\right]
          \end{equation}
\end{enumerate}

Обучение модели с учителем сводится к задаче минимизации функции потерь по тренировочной выборке:

\begin{equation}
    \mathcal{L}_{\mathcal{D}_{\text{train}}}(\theta) = \frac{1}{\lvert \mathcal{D}_{\text{train}} \rvert}\sum_{i=1}^{\lvert \mathcal{D}_{\text{train}} \rvert}\ell(h_{\theta}(x_i),\hat y_i) \rightarrow \min_{\theta}.
\end{equation}

Чтобы решить такую задачу минимизации функции потерь по тренировочной выборке, требуется вычислить:

\begin{equation}
    \frac{\partial \mathcal{L}_{\mathcal{D}_{\text{train}}}(\theta)}{\partial \theta}.
    \label{loss-grad}
\end{equation}

Метод, который позволяет аналитически вычислить градиент (\ref{loss-grad}) в точке, называется методом обратного распостранения ошибки \cite{backprop-theory}. Основа метода -- автоматическое построение графа вычислений и правило вычисления производной сложной функции. С полученным градиентом функции потерь параметры модели ИНС меняются алгоритмом оптимизации. Одними из важных составляющих алгоритмов оптимизации являются выбор размера шага оптимизатора $\eta$, называемым еще скоростью обучения, и планировщик скорости обучения $\varsigma$, поскольку влияют на скорость процесса обучения и на преодоления методом оптимизации локальных минимумов. Частым выбором таких алгоритмов являются: <<\textit{GD}>> (градиентный спуск), <<\textit{SGD}>> (стохастический градиентный спуск), Adam, AdaFactor \cite{optimizers-paper,adafactor-paper}.

Обучение является итеративным процессом, где итерация или шаг итерации -- это обработка моделью одного или нескольких примеров обучающей выборки. Обработка полного набора выборки называют эпохой.

Алгоритм обучения модели ИНС с учителем представлен ниже.

\begin{algorithm}
    \floatname{algorithm}{Алгоритм}
    \caption{Обучение модели ИНС с учителем}
    \begin{algorithmic}[1]
        \State Инициализировать $\theta$ случайно или по некоторому закону распределения.
        \State По каждой эпохе из количества эпох:
        \State\hspace{\algorithmicindent} По каждому примеру $(x, \hat y)$ из обучающей выборки $\mathcal{D}_{\text{train}}$:
        \State\hspace{\algorithmicindent}\hspace{\algorithmicindent} Получить предсказание модели $y \gets h_{\theta}(x)$.
        \State\hspace{\algorithmicindent}\hspace{\algorithmicindent} Получить значение функции потерь $\ell(y, \hat y)$.
        \State\hspace{\algorithmicindent}\hspace{\algorithmicindent} Получить градиент $\nabla \ell$ методом обратного распостранения.
        \State\hspace{\algorithmicindent}\hspace{\algorithmicindent} Сделать шаг оптимизации.
        \State\hspace{\algorithmicindent}\hspace{\algorithmicindent} Аккумулировать значение общей функции потерь $\mathcal{L} \gets \mathcal{L} + \ell(y, \hat y)$.
    \end{algorithmic}
\end{algorithm}

Однако одной тренировочной подвыборки чаще всего не достаточно для успешного обучения модели. Как правило используют три подвыборки исходных данных $\mathcal{D}$. Помимо обучающей, используется валидационная $\mathcal{D}_{\text{val}}$, которая используется в конце эпохи обучения, на которой модель не обучается, но проверяется на наборе данных, которые она не видела, для корректировки гиперпараметров модели. Гиперпараметры -- это параметры, которые используются для контроля процесса обучения. Примерами гиперпараметров могут служить как вышеупомянутые $\eta$ и $\varsigma$, так и количество слоев в модели, активационные функции и т.д. Для оценки итогого качества модели обычно используется тестовая выборка $\mathcal{D}_{\text{test}}$. Методы, которые разбивают исходный набор данных $\mathcal{D}$ на подвыборки, называются методами стратификации.

Хоть $\ell$, $\mathcal{L}$ и показывают качество прогнозирования модели $h_{\theta}$, но на практике анализировать качество модели только по значениям функции потерь -- это сложная задача. Помимо функции потерь используются метрики оценки прогнозирования. Выбор метрик сильно зависит от поставленной задачи.

\section{ИСКУССТВЕННЫЕ НЕЙРОННЫЕ СЕТИ В ОБРАБОТКЕ ЕСТЕСТВЕННОГО ЯЗЫКА}
% TODO: Tokenization, word embeddings
\subsubsection{ВЕКТОРНОЕ ПРЕДСТАВЛЕНИЕ ТЕКСТА}
Из устройства работы ИНС следует, чтобы данные, на которых обучается модель были численными. Поэтому при обработке текста требуется получить его векторное представление для дальнейшей работы с ним.

Простейшим методом представления слов в векторном пространстве является <<\textit{One-Hot Encoding}>> (быстрое кодирование). Его суть заключается в присвоении каждому слову из входной последовательности слов вектора, где в позиции, соотвествующей слову в словаре размерностью словаря, ставится единица, а во всех остальных позициях -- ноль. Словарь содержит весь список возможных слов для кодирования. Размерность такого вектора составляет $1 \times N$, где $N$ -- количество слов в словаре. Пример быстрого кодирования показан ниже.

\begin{table}[H]
    \captionsetup{format=hang, singlelinecheck=false}
    \raggedleft
    \caption{Пример словаря}
    \label{tab:dict}
    \centering
    \begin{tabular}{|p{5cm}|}
        \hline
        \textbf{Цвет} \\
        \hline
        Красный       \\
        \hline
        Зеленый       \\
        \hline
        Синий         \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[H]
    \captionsetup{format=hang, singlelinecheck=false}
    \raggedleft
    \caption{Пример быстрого кодирования}
    \label{tab:ohe}
    \centering
    \begin{tabular}{|p{5cm}|p{5cm}|p{5cm}|}
        \hline
        \textbf{Красный} & \textbf{Зеленый} & \textbf{Синий} \\
        \hline
        1                & 0                & 0              \\
        \hline
        0                & 1                & 0              \\
        \hline
        0                & 0                & 1              \\
        \hline
    \end{tabular}
\end{table}

Представлением текстовых данных в численном виде могут заниматься и модели ИНС: учить полезную информацию о входной последовательности, кодировать ее в векторном виде для последующего использования на конечной задачи, например, задачи классификации или задачи генерации текста. Одной из первых широко распостраненных обученных моделей для кодирования текста является Word2vec \cite{word2vec-paper}.

\subsection{ПОСТАНОВКА ЗАДАЧИ ДИАЛОГОВОЙ СИСТЕМЫ}
% TODO: Dialogue state, dialogue turn, dialogue management
PLACEHOLDER

\section{АРХИТЕКТУРА ТРАНСФОРМЕРА И ЕЕ РОЛЬ В ДИАЛОГОВЫХ МОДЕЛЯХ}
WOW

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{transformer-arch}
    \caption{Архитектура трансформера}
    \label{fig:transformer-arch}
\end{figure}


\subsection{МЕХАНИЗМ ВНИМАНИЯ В ТРАНСФОРМЕРАХ}
PLACEHOLDER

\subsection{МОДЕЛЬ T5: АРХИТЕКТУРА И ЕЕ ОСОБЕННОСТИ}
PLACEHOLDER

\subsection{FLAN}
PLACEHOLDER
