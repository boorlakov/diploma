\begin{thebibliography}{99}
  \addcontentsline{toc}{chapter}{СПИСОК ЛИТЕРАТУРЫ}
  \bibitem{bib1}
  Touvron H. et al. Llama: Open and efficient foundation language models //arXiv preprint arXiv:2302.13971. – 2023.
  \bibitem{bib2}
  Документация Alpaca [Электронный ресурс]: \url{https://crfm.stanford.edu/2023/03/13/alpaca.html}
  \bibitem{bib3}
  Документация ChatGPT [Электронный ресурс]: \url{https://openai.com/blog/chatgpt}
  \bibitem{bib4}
  Бенчмарк MMLU [Электронный ресурс]: \url{https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu}
  \bibitem{bib5}
  Chung H. W. et al. Scaling instruction-finetuned language models //arXiv preprint arXiv:2210.11416. – 2022.
  \bibitem{bib6}
  Raffel C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer //The Journal of Machine Learning Research. – 2020. – Т. 21. – №. 1. – С. 5485-5551.
\end{thebibliography}